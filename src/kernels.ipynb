{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bechmarking Custom Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, Random\n",
    "using Test, BenchmarkTools\n",
    "CUDA.allowscalar(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function subtract!(x, val)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x \n",
    "    @inbounds x[i] -= val\n",
    "    return\n",
    "end\n",
    "\n",
    "a = CUDA.ones(Float64, 256*4)\n",
    "target = a .- 1\n",
    "@cuda threads=256 blocks=4 subtract!(a, 1)\n",
    "all(target .== a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add_gpu!(y, x)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    y[i] += x[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "a = CUDA.rand(Float64, 256*4)\n",
    "b = CUDA.rand(Float64, 256*4)\n",
    "c = a .+ b\n",
    "@cuda threads=256 blocks=4 add_gpu!(b, a)\n",
    "all(b .== c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partition (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"perm_test.jl\")\n",
    "include(\"partition.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 0.73% (59.125 MiB/7.936 GiB)\n",
      "No memory pool is in use."
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set_thread_block (generic function with 2 methods)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function set_thread_block(len, nthreads=256)\n",
    "    nblocks = ceil(Int, len / nthreads)\n",
    "    return nthreads, nblocks\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1 2 … 11 12; 1 2 … 11 13; … ; 8 10 … 19 20; 9 10 … 19 20], [20 19 … 14 13; 20 19 … 14 12; … ; 9 7 … 2 1; 8 7 … 2 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2^15                    # num. samples\n",
    "T, B = set_thread_block(N)\n",
    "nx, ny = 12, 8              # sample sizes for each group\n",
    "x, y = CUDA.rand(Float64, N, nx), CUDA.rand(Float64, N, ny)\n",
    "px, py = partition(nx, ny)\n",
    "px, py = CuArray(px), CuArray(py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean & Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox(out, mean(x, dims = 2))\n",
       "   Evaluated: isapprox([0.39733968302108463, 0.46606242490765926, 0.509690131471409, 0.38030606362527636, 0.6635388999644857, 0.5335489722052404, 0.6207313270377118, 0.5519816092430713, 0.4478335769723505, 0.5470873382976155  …  0.48842900703929043, 0.5114141271222917, 0.5877468897883708, 0.42488719483401827, 0.5967271710540742, 0.448077987777508, 0.5754315354221882, 0.44142439703720476, 0.5227641299077471, 0.4134743813888784], [0.39733968302108463; 0.46606242490765926; … ; 0.5227641299077471; 0.4134743813888784;;])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mean!(x, ncol, out)\n",
    "    \"\"\"out = sum(x, dims=2)\"\"\"\n",
    "    row_idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    for i = 1:ncol\n",
    "        @inbounds out[row_idx] += x[row_idx, i]\n",
    "    end\n",
    "    @inbounds out[row_idx] /= ncol\n",
    "    return\n",
    "end\n",
    "\n",
    "out = CUDA.zeros(Float64, N)\n",
    "@cuda threads=T blocks=B mean!(x, size(x)[2], out)\n",
    "@test isapprox(out, mean(x, dims=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox(out, sum(x .^ 2, dims = 2))\n",
       "   Evaluated: isapprox([3.3505186320850746, 3.5205090438719506, 4.599308140690546, 2.5799348098844064, 6.183259698236766, 4.028304261879531, 5.590323020933638, 4.48616806528492, 3.3989079529358355, 4.538556192110131  …  3.8212513017881085, 3.983841694657771, 4.934117957318372, 3.298106200977595, 5.387028066307541, 3.3676077919729015, 4.902036830006772, 3.254992949429894, 4.460643440418318, 3.0730376887687623], [3.3505186320850746; 3.5205090438719506; … ; 4.460643440418318; 3.0730376887687623;;])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sumsq!(x, ncol, out)\n",
    "    \"\"\"out = sum(x, dims=2)\"\"\"\n",
    "    row_idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    for i = 1:ncol\n",
    "        @inbounds out[row_idx] += x[row_idx, i]^2\n",
    "    end\n",
    "    return\n",
    "end\n",
    "\n",
    "out = CUDA.zeros(Float64, N)\n",
    "@cuda threads=T blocks=B sumsq!(x, size(x)[2], out)\n",
    "@test isapprox(out, sum(x.^2, dims=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox((var_gpu(z))[1], var(z, dims = 2))\n",
       "   Evaluated: isapprox([0.0990023905050552, 0.08724602566531708, 0.09236236634725219, 0.07680292526336481, 0.11297964265478316, 0.09154778136352783, 0.07007101500217339, 0.10159567902516423, 0.08830723936878838, 0.1047530154445524  …  0.07780201021880724, 0.0831995443758919, 0.08626285371574997, 0.08978033242054828, 0.09929088849489377, 0.07658279287466459, 0.05971251101062614, 0.08081223981841738, 0.08140016915085069, 0.09124581895573898], [0.0990023905050551; 0.08724602566531728; … ; 0.08140016915085076; 0.09124581895573891;;])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _var!(n, ss, means, out)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds out[i] = (ss[i] - (n * means[i]^2)) / (n-1)\n",
    "    return\n",
    "end\n",
    "\n",
    "function var_gpu(x)\n",
    "    nrow, ncol = size(x)\n",
    "    T, B = set_thread_block(nrow)\n",
    "    ss = CUDA.zeros(Float64, size(x)[1])\n",
    "    @cuda threads=T blocks=B sumsq!(x, ncol, ss)\n",
    "\n",
    "    means = CUDA.zeros(Float64, nrow)\n",
    "    @cuda threads=T blocks=B mean!(x, ncol, means)\n",
    "\n",
    "    vars = CUDA.zeros(Float64, nrow)\n",
    "    @cuda threads=T blocks=B _var!(ncol, ss, means, vars)\n",
    "    return vars, means\n",
    "end\n",
    "\n",
    "z = CUDA.rand(Float64, 300, 20)  # works with arbitrary matrix size\n",
    "@test isapprox(var_gpu(z)[1], var(z, dims=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t Test Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copy_arr_gpu! (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mul_gpu!(x, val)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] *= val\n",
    "    return\n",
    "end\n",
    "\n",
    "function mul_arr_gpu!(x, y)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] *= y[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function div_gpu!(x, val)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] /= val\n",
    "    return\n",
    "end\n",
    "\n",
    "function div_arr_gpu!(x, y)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] /= y[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function add_gpu!(x, val)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] += val\n",
    "    return\n",
    "end\n",
    "\n",
    "function add_arr_gpu!(x, y)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] += y[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function sub_gpu!(x, val)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] -= val\n",
    "    return\n",
    "end\n",
    "\n",
    "function sub_arr_gpu!(x, y)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] -= y[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function sqrt_gpu!(x)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] = sqrt(x[i])\n",
    "    return\n",
    "end\n",
    "\n",
    "function copy_arr_gpu!(dest, source)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds dest[i] = source[i]\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float32, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.42432487"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "GPU compilation of kernel sub_gpu!(CuDeviceVector{Float64, 1}, CuDeviceVector{Float32, 1}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details.\n",
     "output_type": "error",
     "traceback": [
      "GPU compilation of kernel sub_gpu!(CuDeviceVector{Float64, 1}, CuDeviceVector{Float32, 1}) failed\nKernelError: kernel returns a value of type `Union{}`\n\nMake sure your kernel function ends in `return`, `return nothing` or `nothing`.\nIf the returned value is of type `Union{}`, your Julia code probably throws an exception.\nInspect the code with `@device_code_warntype` for more details.\n",
      "",
      "Stacktrace:",
      "  [1] check_method(job::GPUCompiler.CompilerJob)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/HeCT6/src/validation.jl:21",
      "  [2] macro expansion",
      "    @ ~/.julia/packages/TimerOutputs/YJq3h/src/TimerOutput.jl:252 [inlined]",
      "  [3] macro expansion",
      "    @ ~/.julia/packages/GPUCompiler/HeCT6/src/driver.jl:89 [inlined]",
      "  [4] emit_julia(job::GPUCompiler.CompilerJob)",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/HeCT6/src/utils.jl:64",
      "  [5] cufunction_compile(job::GPUCompiler.CompilerJob)",
      "    @ CUDA ~/.julia/packages/CUDA/sCev8/src/compiler/execution.jl:324",
      "  [6] cached_compilation(cache::Dict{UInt64, Any}, job::GPUCompiler.CompilerJob, compiler::typeof(CUDA.cufunction_compile), linker::typeof(CUDA.cufunction_link))",
      "    @ GPUCompiler ~/.julia/packages/GPUCompiler/HeCT6/src/cache.jl:90",
      "  [7] cufunction(f::typeof(sub_gpu!), tt::Type{Tuple{CuDeviceVector{Float64, 1}, CuDeviceVector{Float32, 1}}}; name::Nothing, kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ CUDA ~/.julia/packages/CUDA/sCev8/src/compiler/execution.jl:297",
      "  [8] cufunction(f::typeof(sub_gpu!), tt::Type{Tuple{CuDeviceVector{Float64, 1}, CuDeviceVector{Float32, 1}}})",
      "    @ CUDA ~/.julia/packages/CUDA/sCev8/src/compiler/execution.jl:291",
      "  [9] top-level scope",
      "    @ ~/.julia/packages/CUDA/sCev8/src/compiler/execution.jl:102",
      " [10] eval",
      "    @ ./boot.jl:373 [inlined]",
      " [11] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "Random.seed!(123)\n",
    "a = CuArray(rand(N))\n",
    "b = copy(a)\n",
    "val = CUDA.rand(1)\n",
    "T, B = set_thread_block(size(a,1))\n",
    "@cuda threads=T blocks=B sub_gpu!(a, val)\n",
    "isapprox(a, b .- val)\n",
    "#@cuda threads=T blocks=B div_gpu!(a, val)\n",
    "#isapprox(a, (b .- val) ./ val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_gpu (generic function with 1 method)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function t_gpu(x, y, pooled)\n",
    "    varx, meanx = var_gpu(x)\n",
    "    vary, meany = var_gpu(y)\n",
    "    nx, ny = size(x, 2), size(y, 2)\n",
    "    T, B = set_thread_block(size(x,1))\n",
    "    return varx\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(var_gpu(x[1,:]')[1] .== var(x[1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox(t_gpu(x, y, pooled), ttest_ind(x, y, pooled))\n",
       "   Evaluated: isapprox([-0.17846062188608072, -0.5544929233801428, -0.053442709686935326, -1.6453844463179053, 0.7985744065403201, 0.6613679526260251, 1.4077764704394224, 0.9316901817580137, 0.976555746673132, -1.0320007794048787  …  -0.1742915694431432, 0.10326542932082894, 1.1239376782276302, -0.7318112449317216, 1.1491076499914104, -0.20567239115503766, 0.17747838757216614, -1.0966573125795651, -0.19199019010346396, -1.497745664028119], [-0.17846062188608072; -0.5544929233801424; … ; -0.19199019010346394; -1.497745664028119;;])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = false\n",
    "@test isapprox(\n",
    "    t_gpu(x, y, pooled),\n",
    "    ttest_ind(x, y, pooled)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " -2.7438546778295443\n",
       " -4.170154062102384"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@views t_gpu(x[1:2,:], y[1:2,:], false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation test p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×12 adjoint(::CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}) with eltype Float64:\n",
       " 0.758347  0.156238  0.00663024  0.761705  …  0.126937  0.0843308  0.776577"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,:]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " -0.1676415669614003"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_ind(x[1,:], y[1,:], pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " NaN"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_gpu(x[1,:]', y[1,:]', false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " NaN"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pval_gpu(x, y, px, py; pooled=false, alternative=\"two-sided\", delta=0)\n",
    "    x_shift = copy(x)\n",
    "    T, B = set_thread_block(size(x,1))\n",
    "    @cuda threads=T blocks=B sub_gpu!(x_shift, delta)\n",
    "    t_obs = t_gpu(x_shift', y', pooled)\n",
    "    return t_obs\n",
    "end\n",
    "\n",
    "pval_gpu(x[1,:], y[1,:], px, py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×12 adjoint(::CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}) with eltype Float64:\n",
       " 0.758347  0.156238  0.00663024  0.761705  …  0.126937  0.0843308  0.776577"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1,:]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    x1_shifted = x1 .- delta             # shift group 1 under null hypothesis\n",
    "    t_obs = ttest_ind(x1_shifted, x2, pooled)  # test statistic for observed data\n",
    "    # println(t_obs)\n",
    "\n",
    "    combined = vcat(x1_shifted, x2)  # join original pair into single vector\n",
    "    x1s = combined[parts1]   # get all combinations of pairs from original pair\n",
    "    x2s = combined[parts2]\n",
    "    ts = ttest_ind(x1s, x2s, pooled)   # test statistic for all possible pairs of samples\n",
    "\n",
    "    if alternative == \"smaller\"\n",
    "        n_extreme = count(ts .<= t_obs)\n",
    "    elseif alternative == \"larger\"\n",
    "        n_extreme = count(ts .>= t_obs)\n",
    "    else\n",
    "        n_extreme = count(@. (ts <= -abs(t_obs)) | (ts >= abs(t_obs)))\n",
    "    end\n",
    "\n",
    "    return n_extreme / size(parts1)[1]  # proportion of pairs w/ extreme test statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t Test Statistic: `@cuda` vs. `@.` vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  248.894 μs (428 allocations: 22.92 KiB)\n",
      "  706.026 μs (340 allocations: 19.33 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32768×1 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       " -0.23448901664480554\n",
       "  0.05901404020274746\n",
       "  0.38193147235097874\n",
       " -1.1025761619685284\n",
       " -0.37842271181044496\n",
       " -0.5640191022990872\n",
       "  0.9204858513562348\n",
       " -0.5540337831883215\n",
       " -2.022682039847497\n",
       " -0.30371442721737457\n",
       "  1.5446552677841405\n",
       " -0.3557875158409345\n",
       " -1.23487851962008\n",
       "  ⋮\n",
       " -0.41275974205144184\n",
       " -1.993862076306121\n",
       " -0.7057823538090785\n",
       " -0.6766744521720237\n",
       "  2.142521777498816\n",
       " -1.7249230335734638\n",
       "  1.0848105576581777\n",
       "  1.2222689568584562\n",
       " -0.5442251380071487\n",
       "  0.11810933367877134\n",
       "  0.5399342464957391\n",
       "  1.2943106132842193"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime t_gpu($x, $y, pooled=$pooled)\n",
    "@btime ttest_ind($x, $y, $pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.525 μs (0 allocations: 0 bytes)\n",
      "  6.128 μs (24 allocations: 1.28 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: all(a .== b)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.rand(N)\n",
    "b = CUDA.zeros(N)\n",
    "\n",
    "@btime copyto!($b, $a)\n",
    "@btime @cuda threads=T blocks=B copy_arr_gpu!(b, a)\n",
    "@test all(a .== b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing `CuArray`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.660 μs (4 allocations: 128 bytes)\n",
      "  12.167 μs (22 allocations: 1.11 KiB)\n",
      "  26.057 μs (7 allocations: 78.31 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime CuArray{Float64, 1}(undef, 10_000)\n",
    "@btime CUDA.zeros(Float64, 10_000)\n",
    "@btime CuArray(zeros(10_000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.7.1",
   "language": "julia",
   "name": "julia-(8-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

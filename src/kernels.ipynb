{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bechmarking Custom Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using CUDA, Random\n",
    "using Test, BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "partition (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"perm_test.jl\")\n",
    "include(\"partition.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effective GPU memory usage: 39.93% (3.169 GiB/7.936 GiB)\n",
      "No memory pool is in use."
     ]
    }
   ],
   "source": [
    "CUDA.memory_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.39015901569917794 0.7847194345068826 … 0.20928740672719975 0.9220333236950535; 0.16428808275350754 0.7056426236607227 … 0.6910468494711057 0.11410357680405231; … ; 0.6185908187952656 0.8206356303474067 … 0.4199970199504272 0.7386790938706924; 0.47547065097259417 0.26788680501761214 … 0.3755500819595143 0.6001599265404269], [0.5027007481726635 0.020998168712858367 … 0.00425394473947821 0.9529314091650041; 0.2140818046135799 0.1042998694251655 … 0.4045272179679979 0.8507021117824134; … ; 0.07906185207522681 0.6921827628559143 … 0.958950005841269 0.510196219231605; 0.695025035001501 0.08335582501523614 … 0.0004320606559524154 0.6036066795632946])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 2^15                       # num. samples\n",
    "T = 256                        # num. threads\n",
    "B = ceil(Int64, N / nthreads)  # num. blocks\n",
    "nx, ny = 12, 8                 # sample sizes for each group\n",
    "x, y = CUDA.rand(Float64, N, nx), CUDA.rand(Float64, N, ny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1 2 … 11 12; 1 2 … 11 13; … ; 8 10 … 19 20; 9 10 … 19 20], [20 19 … 14 13; 20 19 … 14 12; … ; 9 7 … 2 1; 8 7 … 2 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "px, py = partition(nx, ny)\n",
    "px, py = CuArray(px), CuArray(py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
     "output_type": "error",
     "traceback": [
      "Scalar indexing is disallowed.\nInvocation of getindex resulted in scalar indexing of a GPU array.\nThis is typically caused by calling an iterating implementation of a method.\nSuch implementations *do not* execute on the GPU, but very slowly on the CPU,\nand therefore are only permitted from the REPL for prototyping purposes.\nIf you did intend to index this array, annotate the caller with @allowscalar.",
      "",
      "Stacktrace:",
      " [1] error(s::String)",
      "   @ Base ./error.jl:33",
      " [2] assertscalar(op::String)",
      "   @ GPUArrays ~/.julia/packages/GPUArrays/gkF6S/src/host/indexing.jl:53",
      " [3] getindex(xs::CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}, I::Int64)",
      "   @ GPUArrays ~/.julia/packages/GPUArrays/gkF6S/src/host/indexing.jl:86",
      " [4] top-level scope",
      "   @ In[40]:2",
      " [5] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [6] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "CUDA.allowscalar(false)\n",
    "x[1]   # scalar indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125970×12 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       " 0.390159  0.164288   0.048688   0.277681  …  0.0676202  0.253002   0.762648\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.17261\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.742003\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.118479\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.385812\n",
       " 0.390159  0.164288   0.048688   0.277681  …  0.0676202  0.253002   0.476971\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.766175\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.0718647\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.253002   0.904703\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.762648   0.17261\n",
       " 0.390159  0.164288   0.048688   0.277681  …  0.0676202  0.762648   0.742003\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.762648   0.118479\n",
       " 0.390159  0.164288   0.048688   0.277681     0.0676202  0.762648   0.385812\n",
       " ⋮                                         ⋱             ⋮          \n",
       " 0.667844  0.535361   0.0676202  0.253002     0.476971   0.766175   0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002     0.476971   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002  …  0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002     0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002     0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002     0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002     0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.253002  …  0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.0676202  0.762648     0.766175   0.0718647  0.904703\n",
       " 0.667844  0.535361   0.253002   0.762648     0.766175   0.0718647  0.904703\n",
       " 0.667844  0.0676202  0.253002   0.762648     0.766175   0.0718647  0.904703\n",
       " 0.535361  0.0676202  0.253002   0.762648     0.766175   0.0718647  0.904703"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[px]  # matrix (non-scalar) indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean & Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mean! (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mean!(x, n, out)\n",
    "    \"\"\"out = sum(x, dims=2)\"\"\"\n",
    "    row_idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    for i = 1:n\n",
    "        @inbounds out[row_idx] += x[row_idx, i]\n",
    "    end\n",
    "    @inbounds out[row_idx] /= n\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox(out, mean(x, dims = 2))\n",
       "   Evaluated: isapprox([0.42276973218318964, 0.42767573385618834, 0.4831524712592485, 0.48611237979911487, 0.4395516192435302, 0.49092161596331424, 0.4851448484475114, 0.5351914380198286, 0.3874865059055883, 0.44487627001643726  …  0.43060517340148524, 0.34602936197057826, 0.6835004180168199, 0.49907858337740735, 0.499260425583421, 0.6306563601147931, 0.49835490460993603, 0.4185987769980128, 0.5069556192809072, 0.4936965027742839], [0.42276973218318964; 0.42767573385618834; … ; 0.5069556192809072; 0.4936965027742839;;])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = CUDA.zeros(Float64, N)\n",
    "@cuda threads=T blocks=B mean!(x, size(x)[2], out)\n",
    "@test isapprox(out, mean(x, dims=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sumsq! (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sumsq!(x, n, out)\n",
    "    \"\"\"out = sum(x, dims=2)\"\"\"\n",
    "    row_idx = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    for i = 1:n\n",
    "        @inbounds out[row_idx] += x[row_idx, i]^2\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox(out, sum(x .^ 2, dims = 2))\n",
       "   Evaluated: isapprox([2.9531018774148086, 3.1790675748369743, 3.9656508444115333, 3.800037022907106, 3.356273550543609, 4.16561891875466, 4.0703096558270575, 4.111705031998799, 2.866273597668395, 3.415905663621952  …  3.142739218575019, 2.6207189658821237, 6.347311199904543, 3.6258463421240386, 3.8745421141461165, 5.555892979579336, 4.239972555346566, 2.9453676018286834, 3.572019126119588, 3.5529036727134033], [2.9531018774148086; 3.1790675748369743; … ; 3.5720191261195877; 3.552903672713403;;])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = CUDA.zeros(Float64, N)\n",
    "@cuda threads=T blocks=B sumsq!(x, size(x)[2], out)\n",
    "@test isapprox(out, sum(x.^2, dims=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "var_gpu (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function _var!(n, ss, means, out)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds out[i] = (ss[i] - (n * means[i]^2)) / (n-1)\n",
    "    return\n",
    "end\n",
    "\n",
    "function var_gpu(x)\n",
    "    nrow, ncol = size(x)\n",
    "    ss = CUDA.zeros(Float64, size(x)[1])\n",
    "    @cuda threads=T blocks=B sumsq!(x, ncol, ss)\n",
    "\n",
    "    means = CUDA.zeros(Float64, nrow)\n",
    "    @cuda threads=T blocks=B mean!(x, ncol, means)\n",
    "\n",
    "    vars = CUDA.zeros(Float64, nrow)\n",
    "    @cuda threads=T blocks=B _var!(ncol, ss, means, vars)\n",
    "    return vars, means\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox((var_gpu(x))[1], var(x, dims = 2))\n",
       "   Evaluated: isapprox([0.07348099272835072, 0.08947174317125667, 0.1058559198731369, 0.0876703703072363, 0.0943460035262088, 0.11577913841038899, 0.11326576073865019, 0.06132241164018315, 0.09677491732375788, 0.09462971964882692  …  0.08342631220486738, 0.10762573943026708, 0.06738521297768471, 0.0578993775901856, 0.08031004031829571, 0.07119669499458733, 0.11451647490556587, 0.07660621532526354, 0.04436101155215741, 0.0570971664086092], [0.07348099272835071; 0.08947174317125672; … ; 0.04436101155215735; 0.05709716640860912;;])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@test isapprox(var_gpu(x)[1], var(x, dims=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t Test Statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "copy_arr_gpu! (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mul_gpu!(x, val)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] *= val\n",
    "    return\n",
    "end\n",
    "\n",
    "function mul_arr_gpu!(x, y)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] *= y[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function div_gpu!(x, val)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] /= val\n",
    "    return\n",
    "end\n",
    "\n",
    "function div_arr_gpu!(x, y)\n",
    "    \"\"\" out = x ./ y \"\"\"\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] /= y[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function sqrt_gpu!(x)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds x[i] = sqrt(x[i])\n",
    "    return\n",
    "end\n",
    "\n",
    "function add_gpu!(a, b, c)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds c[i] = a[i] + b[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function subtract_gpu!(a, b, c)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds c[i] = a[i] - b[i]\n",
    "    return\n",
    "end\n",
    "\n",
    "function copy_arr_gpu!(dest, source)\n",
    "    i = (blockIdx().x-1) * blockDim().x + threadIdx().x\n",
    "    @inbounds dest[i] = source[i]\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t_gpu (generic function with 1 method)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function t_gpu(x, y; pooled=false)\n",
    "    varx, meanx = var_gpu(x)\n",
    "    vary, meany = var_gpu(y)\n",
    "    nx, ny = size(x)[2], size(y)[2]\n",
    "    \n",
    "    if pooled\n",
    "        @cuda threads=T blocks=B mul_gpu!(varx, nx-1)\n",
    "        @cuda threads=T blocks=B mul_gpu!(vary, ny-1)\n",
    "        @cuda threads=T blocks=B add_gpu!(varx, vary, varx)   # varx = (nx-1)*varx + (ny-1)*vary\n",
    "        @cuda threads=T blocks=B div_gpu!(varx, nx+ny-2)      # varx /= nx+ny-2\n",
    "        #@cuda threads=256 blocks=4 copy_arr_gpu!(vary, varx)\n",
    "        copyto!(vary, varx)\n",
    "    end\n",
    "        \n",
    "    @cuda threads=T blocks=B div_gpu!(varx, nx)\n",
    "    @cuda threads=T blocks=B div_gpu!(vary, ny)\n",
    "    \n",
    "    denom = CUDA.zeros(Float64, size(x)[1])\n",
    "    @cuda threads=T blocks=B add_gpu!(varx, vary, denom)\n",
    "    @cuda threads=T blocks=B sqrt_gpu!(denom)\n",
    "    \n",
    "    t = CUDA.zeros(Float64, size(x)[1])\n",
    "    @cuda threads=T blocks=B subtract_gpu!(meanx, meany, t)\n",
    "    @cuda threads=T blocks=B div_arr_gpu!(t, denom)\n",
    "    return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: isapprox(t_gpu(x, y, pooled = pooled), ttest_ind(x, y, pooled))\n",
       "   Evaluated: isapprox([-0.23448901664480554, 0.05901404020274747, 0.38193147235097874, -1.1025761619685288, -0.37842271181044485, -0.5640191022990872, 0.9204858513562348, -0.5540337831883215, -2.022682039847498, -0.30371442721737474  …  -0.7057823538090785, -0.6766744521720237, 2.142521777498816, -1.7249230335734647, 1.084810557658178, 1.222268956858456, -0.5442251380071487, 0.11810933367877136, 0.5399342464957391, 1.2943106132842188], [-0.23448901664480554; 0.05901404020274746; … ; 0.5399342464957391; 1.2943106132842193;;])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pooled = false\n",
    "@test isapprox(t_gpu(x, y, pooled=pooled), ttest_ind(x, y, pooled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t Test Statistic: `@cuda` vs. `@.` vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  248.894 μs (428 allocations: 22.92 KiB)\n",
      "  706.026 μs (340 allocations: 19.33 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32768×1 CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}:\n",
       " -0.23448901664480554\n",
       "  0.05901404020274746\n",
       "  0.38193147235097874\n",
       " -1.1025761619685284\n",
       " -0.37842271181044496\n",
       " -0.5640191022990872\n",
       "  0.9204858513562348\n",
       " -0.5540337831883215\n",
       " -2.022682039847497\n",
       " -0.30371442721737457\n",
       "  1.5446552677841405\n",
       " -0.3557875158409345\n",
       " -1.23487851962008\n",
       "  ⋮\n",
       " -0.41275974205144184\n",
       " -1.993862076306121\n",
       " -0.7057823538090785\n",
       " -0.6766744521720237\n",
       "  2.142521777498816\n",
       " -1.7249230335734638\n",
       "  1.0848105576581777\n",
       "  1.2222689568584562\n",
       " -0.5442251380071487\n",
       "  0.11810933367877134\n",
       "  0.5399342464957391\n",
       "  1.2943106132842193"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime t_gpu($x, $y, pooled=$pooled)\n",
    "@btime ttest_ind($x, $y, $pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.525 μs (0 allocations: 0 bytes)\n",
      "  6.128 μs (24 allocations: 1.28 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: all(a .== b)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = CUDA.rand(N)\n",
    "b = CUDA.zeros(N)\n",
    "\n",
    "@btime copyto!($b, $a)\n",
    "@btime @cuda threads=T blocks=B copy_arr_gpu!(b, a)\n",
    "@test all(a .== b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing `CuArray`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.660 μs (4 allocations: 128 bytes)\n",
      "  12.167 μs (22 allocations: 1.11 KiB)\n",
      "  26.057 μs (7 allocations: 78.31 KiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000-element CuArray{Float64, 1, CUDA.Mem.DeviceBuffer}:\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " ⋮\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@btime CuArray{Float64, 1}(undef, 10_000)\n",
    "@btime CUDA.zeros(Float64, 10_000)\n",
    "@btime CuArray(zeros(10_000))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia (8 threads) 1.7.1",
   "language": "julia",
   "name": "julia-(8-threads)-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
